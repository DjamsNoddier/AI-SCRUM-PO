"""
audio_transcriber.py
--------------------
Transcrit un feedback audio (voix) en texte clair,
segmente automatiquement la discussion par th√®mes (clustering s√©mantique),
puis g√©n√®re une ou plusieurs User Stories par id√©e d√©tect√©e.

Fait partie du projet : AI Scrum PO Assistant
Auteur : Djamil
"""

import os
import re
import json
from pathlib import Path
from difflib import SequenceMatcher
from dotenv import load_dotenv
from functools import lru_cache
from groq import Groq

from .consolidator import consolidate_user_stories
from .generator import generate_user_story, generate_short_title
from .jira_client import export_user_stories_to_jira
from .logger_manager import info, warn, error


# -------------------------
# ‚öôÔ∏è Chargement du .env
# -------------------------
ROOT_DIR = Path(__file__).resolve().parents[2]
ENV_PATH = ROOT_DIR / ".env"

if ENV_PATH.exists():
    load_dotenv(dotenv_path=ENV_PATH)
else:
    warn(f"Fichier .env non trouv√© √† {ENV_PATH}")


# -------------------------
# üß† Initialisation paresseuse du client Groq
# -------------------------
@lru_cache()
def get_groq_client():
    """Initialise le client Groq une seule fois et le met en cache."""
    api_key = os.getenv("GROQ_API_KEY")
    if not api_key:
        raise RuntimeError("‚ùå GROQ_API_KEY manquant. V√©rifie ton fichier .env √† la racine du projet.")
    return Groq(api_key=api_key)


# -------------------------
# üîß Helper JSON robuste
# -------------------------
def _extract_json_block(raw: str, context: str) -> dict:
    """
    Tente d'extraire un vrai JSON depuis la r√©ponse du mod√®le.
    G√®re les cas :
    - ```json ... ```
    - texte avant/apr√®s le JSON
    """
    if not raw:
        raise ValueError("R√©ponse vide")

    txt = raw.strip()

    # Cas 1 : bloc markdown ```json ... ```
    if txt.startswith("```"):
        # on enl√®ve ```json / ``` et on garde l'int√©rieur
        txt = re.sub(r"^```[a-zA-Z]*\s*", "", txt)
        txt = re.sub(r"```$", "", txt.strip()).strip()

    # Cas 2 : il y a du texte autour, on r√©cup√®re entre le 1er { et le dernier }
    start = txt.find("{")
    end = txt.rfind("}")
    if start != -1 and end != -1 and end > start:
        txt = txt[start:end + 1]

    try:
        return json.loads(txt)
    except json.JSONDecodeError as e:
        # Log d√©taill√© pour debug, mais on ne fait pas planter le backend
        warn(f"Erreur parsing JSON ({context})", error=str(e), raw_preview=txt[:300])
        raise


# -------------------------
# üß∞ Nettoyage / d√©duplication
# -------------------------
def _normalize(txt: str) -> str:
    return re.sub(r"\s+", " ", txt.strip().lower())

def _similar(a: str, b: str) -> float:
    return SequenceMatcher(None, _normalize(a), _normalize(b)).ratio()

def dedupe_keep_order(items: list[str], threshold: float = 0.88) -> list[str]:
    """Supprime les doublons tout en gardant l‚Äôordre logique."""
    out = []
    for it in items:
        if not it or len(it.strip()) < 5:
            continue
        if not any(_similar(it, x) >= threshold for x in out):
            out.append(it.strip())
    return out


# -------------------------
# üéß Transcription Audio ‚Üí Texte
# -------------------------
def transcribe_audio(file_path: str) -> str:
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"‚ùå Fichier introuvable : {file_path}")

    with open(file_path, "rb") as audio_file:
        client = get_groq_client()
        response = client.audio.transcriptions.create(
            model="whisper-large-v3-turbo",
            file=audio_file
        )

    text = response.text.strip()
    info("Transcription termin√©e", file=file_path, word_count=len(text.split()))
    return text


# -------------------------
# üß© Segmentation de la conversation
# -------------------------
def segment_conversation_llm(transcribed_text: str) -> list[dict]:
    """D√©coupe le texte transcrit en segments th√©matiques exploitables pour le backlog."""
    prompt = f"""
Tu es un facilitateur d'atelier produit.
D√©coupe le texte suivant en 3 √† 8 segments logiques,
chacun correspondant √† un th√®me produit coh√©rent.

- Ignore les salutations, transitions, ou phrases hors-sujet.
- Donne pour chaque segment :
  - "theme": titre court (max 8 mots)
  - "content": le texte coh√©rent du segment
- R√©ponds STRICTEMENT en JSON valide :
{{"segments":[{{"theme":"...","content":"..."}}]}}.

Texte :
\"\"\"{transcribed_text}\"\"\"
"""
    try:
        client = get_groq_client()
        response = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {"role": "system", "content": "R√©ponds uniquement en JSON valide, sans texte hors JSON."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.25,
        )
        raw = response.choices[0].message.content.strip()
        data = _extract_json_block(raw, context="segmentation")
        return [
            {"theme": s["theme"].strip(), "content": s["content"].strip()}
            for s in data.get("segments", [])
            if s.get("content") and len(s["content"].split()) > 5
        ]
    except Exception as e:
        error("Erreur segmentation conversation", error=str(e))
        return [{"theme": "Discussion g√©n√©rale", "content": transcribed_text}]


# -------------------------
# üß† D√©tection du contenu produit
# -------------------------
def is_segment_about_product(segment_text: str) -> bool:
    """V√©rifie si un segment contient une discussion produit r√©elle."""
    prompt = f"""
Dis seulement "oui" ou "non".
R√©ponds "oui" si ce texte contient une discussion produit :
fonctionnalit√©s, probl√®mes utilisateurs, id√©es d'am√©lioration,
besoins m√©tier, ou retours sur un produit existant.
Sinon, r√©ponds "non".

Texte :
\"\"\"{segment_text}\"\"\"
"""
    client = get_groq_client()
    response = client.chat.completions.create(
        model="llama-3.3-70b-versatile",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
    )
    return "oui" in response.choices[0].message.content.lower()


# -------------------------
# üß© Extraction d‚Äôid√©es produit
# -------------------------
def extract_ideas_from_segment(segment_text: str) -> list[dict]:
    """Extrait les besoins produit explicites et implicites du segment."""
    prompt = f"""
Tu es un Product Manager senior assistant √† un atelier produit.
Analyse ce segment et identifie les besoins produit exprim√©s (ou implicites).
Ignore le bruit conversationnel.

Retourne STRICTEMENT en JSON :
{{"ideas":[{{"idea":"...","title":"...","why":"...","confidence":0.0}}]}}
Si aucune id√©e produit n'est trouv√©e : {{"ideas":[]}}

Segment :
\"\"\"{segment_text}\"\"\"
"""
    try:
        client = get_groq_client()
        response = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {"role": "system", "content": "Tu es un Product Manager exp√©riment√©. R√©ponds UNIQUEMENT en JSON valide."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
        )
        raw = response.choices[0].message.content.strip()
        data = _extract_json_block(raw, context="extraction_id√©es")
        return [i for i in data.get("ideas", []) if i.get("idea") and i.get("confidence", 0) >= 0.5]
    except Exception as e:
        warn("Erreur extraction d‚Äôid√©es", error=str(e))
        return []


# -------------------------
# üìä Scoring de la qualit√© globale
# -------------------------
def compute_us_quality_score(user_stories: list[dict]) -> dict:
    """√âvalue la qualit√© globale des User Stories g√©n√©r√©es."""
    if not user_stories:
        return {"confidence": 0, "diversity": 0, "pertinence": 0, "global_score": 0}

    confidences = [us.get("confidence", 0) for us in user_stories]
    avg_confidence = sum(confidences) / len(confidences) if confidences else 0
    themes = {us["theme"] for us in user_stories if us.get("theme")}
    diversity = len(themes) / len(user_stories)
    valid_us = [us for us in user_stories if us.get("title") and us.get("acceptance_criteria")]
    pertinence = len(valid_us) / len(user_stories)
    global_score = round((avg_confidence * 0.5 + diversity * 0.3 + pertinence * 0.2), 2)

    return {
        "confidence": round(avg_confidence, 2),
        "diversity": round(diversity, 2),
        "pertinence": round(pertinence, 2),
        "global_score": global_score
    }


# -------------------------
# üöÄ Pipeline complet : audio ‚Üí US + summaries
# -------------------------
def process_audio_feedback(file_path: str, push_to_jira: bool = False):
    """Pipeline principal complet"""
    info("Pipeline IA d√©marr√©", file=file_path)

    # √âtape 1 : transcription
    text = transcribe_audio(file_path)
    info("Transcription termin√©e", word_count=len(text.split()))

    # √âtape 2 : segmentation
    segments = segment_conversation_llm(text)
    info("Segmentation effectu√©e", segments_count=len(segments))

    user_stories: list[dict] = []

    # √âtape 3 : boucle segment ‚Üí id√©es
    for idx, seg in enumerate(segments, 1):
        info("Traitement segment", index=idx, theme=seg["theme"])

        if not is_segment_about_product(seg["content"]):
            warn("Segment ignor√© (non-produit)", theme=seg["theme"])
            continue

        ideas = extract_ideas_from_segment(seg["content"])
        if not ideas:
            warn("Aucune id√©e d√©tect√©e", theme=seg["theme"])
            continue

        for idea in ideas[:2]:
            story = generate_user_story(idea["idea"])
            short_title = generate_short_title(story["user_story"])
            enriched = {
                "theme": seg["theme"],
                "idea": idea["idea"],
                "title": short_title,
                "why": idea.get("why", ""),
                "confidence": idea.get("confidence", 0),
                **story
            }
            user_stories.append(enriched)

    # √âtape 4 : consolidation finale
    before = len(user_stories)
    user_stories = consolidate_user_stories(user_stories, threshold=0.8)
    after = len(user_stories)
    info("Consolidation termin√©e", before=before, after=after)

    # √âtape 5 : export Jira
    if push_to_jira and user_stories:
        info("Export Jira activ√©", count=len(user_stories))
        export_user_stories_to_jira(user_stories)
    else:
        info("Export Jira d√©sactiv√©")

    # √âtape 6 : scoring qualit√©
    quality = compute_us_quality_score(user_stories)
    info("Qualit√© √©valu√©e", **quality)

    # √âtape 7 : r√©sum√© meeting (m√™me sans US)
    meeting_summary = summarize_meeting(text)

    # √âtape 8 : r√©sum√© consultant premium
    consulting_summary = generate_consulting_summary(text, user_stories)

    # R√©sum√© global
    info(
        "Pipeline IA termin√©",
        file=file_path,
        segments=len(segments),
        user_stories=len(user_stories)
    )

    return {
        "transcription": text,          # ‚¨ÖÔ∏è utilis√© par le backend
        "user_stories": user_stories,
        "segments": segments,
        "quality": quality,
        "meeting_summary": meeting_summary,
        "consulting_summary": consulting_summary,
    }


# -------------------------
# üßæ R√©sum√© meeting structur√©
# -------------------------
def summarize_meeting(transcribed_text: str) -> dict:
    """
    R√©sume le meeting entier et extrait :
    - contexte
    - points cl√©s
    - d√©cisions
    - risques
    - next steps
    M√™me si aucune user story n‚Äôa √©t√© trouv√©e.
    """

    prompt = f"""
Tu es un assistant IA sp√©cialis√© dans les r√©unions produit.

√Ä partir de ce texte transcrit, produis un r√©sum√© structur√©.
M√™me si le texte n‚Äôest pas tr√®s clair, fais de ton mieux pour remplir les sections.

Retourne STRICTEMENT en JSON :
{{
  "context": "...",
  "key_points": ["...", "..."],
  "decisions": ["...", "..."],
  "risks": ["...", "..."],
  "next_steps": ["...", "..."]
}}

Texte :
\"\"\"{transcribed_text}\"\"\"
"""

    try:
        client = get_groq_client()
        response = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {"role": "system", "content": "Tu es un expert en analyse de r√©union. R√©ponds uniquement en JSON valide."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
        )

        raw = response.choices[0].message.content.strip()
        data = _extract_json_block(raw, context="resume_meeting")
        return {
            "context": data.get("context", ""),
            "key_points": data.get("key_points", []) or [],
            "decisions": data.get("decisions", []) or [],
            "risks": data.get("risks", []) or [],
            "next_steps": data.get("next_steps", []) or [],
        }

    except Exception as e:
        warn("Erreur r√©sum√© meeting", error=str(e))
        return {
            "context": "",
            "key_points": [],
            "decisions": [],
            "risks": [],
            "next_steps": []
        }


# -------------------------
# üíº R√©sum√© consultant (premium)
# -------------------------
def generate_consulting_summary(transcribed_text: str, user_stories: list[dict]) -> dict:
    """
    G√©n√®re un r√©sum√© premium style consultant (McKinsey-like).
    S'appuie sur : transcription brute + US g√©n√©r√©es.
    Retourne un bloc JSON structur√©.
    """

    prompt = f"""
Tu es un consultant senior (McKinsey / BCG).
Produis un r√©sum√© PREMIUM du meeting.

Utilise :
- la transcription brute
- les user stories ci-dessous

User Stories d√©tect√©es :
{json.dumps(user_stories, ensure_ascii=False, indent=2)}

Transcription :
\"\"\"{transcribed_text}\"\"\"

Structure attendue STRICTEMENT en JSON valide :
{{
  "context": "2-3 lignes claires r√©sumant le sujet du meeting",
  "key_points": ["point cl√© 1", "point cl√© 2", ...],
  "decisions": ["d√©cision 1", "d√©cision 2"],
  "risks": ["risque 1", "risque 2"],
  "next_steps": ["action 1", "action 2", "action 3"]
}}

Rappels :
- pas de texte hors JSON
- √©cris des phrases concises, orient√©es action
- bullet points courts
"""

    try:
        client = get_groq_client()
        response = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {
                    "role": "system",
                    "content": "Tu es un consultant McKinsey. Livrable ultra clair. R√©ponds uniquement en JSON."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.3,
        )

        raw = response.choices[0].message.content.strip()
        data = _extract_json_block(raw, context="consulting_summary")

        return {
            "context": data.get("context", ""),
            "key_points": data.get("key_points", []) or [],
            "decisions": data.get("decisions", []) or [],
            "risks": data.get("risks", []) or [],
            "next_steps": data.get("next_steps", []) or [],
        }

    except Exception as e:
        warn("Erreur r√©sum√© consultant", error=str(e))
        return {
            "context": "",
            "key_points": [],
            "decisions": [],
            "risks": [],
            "next_steps": []
        }
