"""
audio_transcriber.py
--------------------
Transcrit un feedback audio (voix) en texte clair,
segmente automatiquement la discussion par thÃ¨mes (clustering sÃ©mantique),
puis gÃ©nÃ¨re une ou plusieurs User Stories par idÃ©e dÃ©tectÃ©e.
Peut enfin les exporter automatiquement vers Jira.

Fait partie du projet : AI Scrum PO Assistant
Auteur : Djamil
"""

import os
import re
import json
from pathlib import Path
from difflib import SequenceMatcher
from dotenv import load_dotenv
from groq import Groq
from .consolidator import consolidate_user_stories
from .generator import generate_user_story, generate_short_title
from .jira_client import export_user_stories_to_jira


# -------------------------
# âš™ï¸ Initialisation
# -------------------------
env_path = Path(__file__).resolve().parent.parent / ".env"
load_dotenv(dotenv_path=env_path)
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# -------------------------
# ğŸ§° Nettoyage / dÃ©duplication
# -------------------------
def _normalize(txt: str) -> str:
    return re.sub(r"\s+", " ", txt.strip().lower())

def _similar(a: str, b: str) -> float:
    return SequenceMatcher(None, _normalize(a), _normalize(b)).ratio()

def dedupe_keep_order(items: list[str], threshold: float = 0.88) -> list[str]:
    """Supprime les doublons tout en gardant lâ€™ordre logique."""
    out = []
    for it in items:
        if not it or len(it.strip()) < 5:
            continue
        if not any(_similar(it, x) >= threshold for x in out):
            out.append(it.strip())
    return out


# -------------------------
# ğŸ§ Transcription Audio â†’ Texte
# -------------------------
def transcribe_audio(file_path: str) -> str:
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"âŒ Fichier introuvable : {file_path}")

    with open(file_path, "rb") as audio_file:
        response = client.audio.transcriptions.create(
            model="whisper-large-v3-turbo",
            file=audio_file
        )

    text = response.text.strip()
    print(f"ğŸ™ï¸ Transcription terminÃ©e : {len(text.split())} mots dÃ©tectÃ©s")
    return text


# -------------------------
# ğŸ§© Segmentation de la conversation
# -------------------------
def segment_conversation_llm(transcribed_text: str) -> list[dict]:
    """
    DÃ©coupe le texte transcrit en segments thÃ©matiques exploitables pour le backlog.
    """
    prompt = f"""
Tu es un facilitateur d'atelier produit.
DÃ©coupe le texte suivant en 3 Ã  8 segments logiques,
chacun correspondant Ã  un thÃ¨me produit cohÃ©rent.

- Ignore les salutations, transitions, ou phrases hors-sujet.
- Donne pour chaque segment :
  - "theme": titre court (max 8 mots)
  - "content": le texte cohÃ©rent du segment
- RÃ©ponds STRICTEMENT en JSON valide au format :
{{"segments":[{{"theme":"...","content":"..."}}]}}.

Texte :
\"\"\"{transcribed_text}\"\"\"
"""
    try:
        response = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {"role": "system", "content": "RÃ©ponds uniquement en JSON valide, sans texte hors JSON."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.25,
        )
        raw = response.choices[0].message.content.strip()
        data = json.loads(raw)
        return [
            {"theme": s["theme"].strip(), "content": s["content"].strip()}
            for s in data.get("segments", [])
            if s.get("content") and len(s["content"].split()) > 5
        ]
    except Exception:
        # fallback simple : 1 segment global
        return [{"theme": "Discussion gÃ©nÃ©rale", "content": transcribed_text}]


# -------------------------
# ğŸ§  DÃ©tection du contenu produit
# -------------------------
def is_segment_about_product(segment_text: str) -> bool:
    """VÃ©rifie si un segment contient une discussion produit rÃ©elle."""
    prompt = f"""
Dis seulement "oui" ou "non".

RÃ©ponds "oui" si ce texte contient une discussion produit :
fonctionnalitÃ©s, problÃ¨mes utilisateurs, idÃ©es d'amÃ©lioration,
besoins mÃ©tier, ou retours sur un produit existant.
Sinon, rÃ©ponds "non".

Texte :
\"\"\"{segment_text}\"\"\"
"""
    response = client.chat.completions.create(
        model="llama-3.3-70b-versatile",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
    )
    return "oui" in response.choices[0].message.content.lower()


# -------------------------
# ğŸ§© Extraction dâ€™idÃ©es produit
# -------------------------
def extract_ideas_from_segment(segment_text: str) -> list[dict]:
    """
    Extrait les besoins produit explicites et implicites du segment.
    Retourne une liste structurÃ©e d'idÃ©es (JSON).
    """
    prompt = f"""
Tu es un Product Manager senior assistant Ã  un atelier produit.
Analyse ce segment et identifie les besoins produit exprimÃ©s (ou implicites).
Ignore le bruit conversationnel.

Retourne STRICTEMENT en JSON :
{{
  "ideas": [
    {{
      "idea": "besoin ou problÃ¨me dÃ©tectÃ©",
      "title": "titre court et clair",
      "why": "raison ou objectif du besoin",
      "confidence": 0.0â€“1.0
    }}
  ]
}}

Si aucune idÃ©e produit n'est trouvÃ©e : {{"ideas":[]}}

Segment :
\"\"\"{segment_text}\"\"\"
"""
    try:
        response = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {"role": "system", "content": "Tu es un Product Manager expÃ©rimentÃ©. RÃ©ponds UNIQUEMENT en JSON valide."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
        )
        data = json.loads(response.choices[0].message.content.strip())
        return [
            i for i in data.get("ideas", [])
            if i.get("idea") and i.get("confidence", 0) >= 0.5
        ]
    except Exception:
        return []

# -------------------------
# ğŸ“Š Scoring de la qualitÃ© globale
# -------------------------
def compute_us_quality_score(user_stories: list[dict]) -> dict:
    """
    Ã‰value la qualitÃ© globale des User Stories gÃ©nÃ©rÃ©es :
    - confiance moyenne (si disponible dans les idÃ©es)
    - diversitÃ© (thÃ¨mes distincts / total)
    - ratio pertinence (titre + critÃ¨res non vides)
    """
    if not user_stories:
        return {"confidence": 0, "diversity": 0, "pertinence": 0, "global_score": 0}

    # Moyenne des scores de confiance
    confidences = [us.get("confidence", 0) for us in user_stories]
    avg_confidence = sum(confidences) / len(confidences) if confidences else 0

    # DiversitÃ© thÃ©matique
    themes = {us["theme"] for us in user_stories if us.get("theme")}
    diversity = len(themes) / len(user_stories)

    # Pertinence basique : titre et critÃ¨res non vides
    valid_us = [
        us for us in user_stories
        if us.get("title") and us.get("acceptance_criteria")
    ]
    pertinence = len(valid_us) / len(user_stories)

    # Score global pondÃ©rÃ©
    global_score = round((avg_confidence * 0.5 + diversity * 0.3 + pertinence * 0.2), 2)

    return {
        "confidence": round(avg_confidence, 2),
        "diversity": round(diversity, 2),
        "pertinence": round(pertinence, 2),
        "global_score": global_score
    }


# -------------------------
# ğŸš€ Pipeline complet : audio â†’ US
# -------------------------
def process_audio_feedback(file_path: str, push_to_jira: bool = False):
    """Pipeline principal complet"""
    # Ã‰tape 1 : transcription
    text = transcribe_audio(file_path)
    print("\nğŸ§  Texte transcrit :")
    print(text[:400] + ("..." if len(text) > 400 else ""))

    # Ã‰tape 2 : segmentation
    print("\nğŸ§© Segmentation de la conversation...")
    segments = segment_conversation_llm(text)
    print(f"âœ… {len(segments)} segment(s) dÃ©tectÃ©(s).\n")

    user_stories = []

    # Ã‰tape 3 : boucle segment â†’ idÃ©es
    for idx, seg in enumerate(segments, 1):
        print(f"ğŸ¯ Segment {idx}/{len(segments)} â€” ThÃ¨me : {seg['theme']}")
        if not is_segment_about_product(seg["content"]):
            print("ğŸ—¨ï¸ Segment conversationnel ignorÃ©.\n")
            continue

        ideas = extract_ideas_from_segment(seg["content"])
        if not ideas:
            print("âš ï¸ Aucun besoin dÃ©tectÃ© dans ce segment.\n")
            continue

        print(f"ğŸ’¡ {len(ideas)} idÃ©e(s) pertinentes dÃ©tectÃ©es :")
        for idea in ideas[:2]:  # max 2 idÃ©es/segment pour Ã©viter le spam
            print(f"   â†’ {idea['title']} ({idea['confidence']:.2f})")

            story = generate_user_story(idea["idea"])
            short_title = generate_short_title(story["user_story"])

            enriched = {
                "theme": seg["theme"],
                "idea": idea["idea"],
                "title": short_title,
                "why": idea.get("why", ""),
                "confidence": idea.get("confidence", 0),
                **story
            }
            user_stories.append(enriched)
            print(f"âœ… {short_title} â†’ {story['user_story']}\n")

    # Ã‰tape 4 : consolidation finale
    print("\nğŸ” Consolidation des User Stories similaires...")
    before = len(user_stories)
    user_stories = consolidate_user_stories(user_stories, threshold=0.8)
    after = len(user_stories)
    print(f"âœ… {before - after} fusion(s), {after} User Stories finales.\n")

    # Ã‰tape 5 : export Jira
    if push_to_jira and user_stories:
        print("ğŸš€ Export vers Jira...")
        export_user_stories_to_jira(user_stories)
    else:
        print("â„¹ï¸ Export Jira dÃ©sactivÃ©.")

        # Ã‰valuation de la qualitÃ©
    print("\nğŸ“Š Ã‰valuation de la qualitÃ© des User Stories...")
    quality = compute_us_quality_score(user_stories)
    print(f"   - Confiance moyenne : {quality['confidence']:.2f}")
    print(f"   - DiversitÃ© thÃ©matique : {quality['diversity']:.2f}")
    print(f"   - Pertinence : {quality['pertinence']:.2f}")
    print(f"   ğŸ‘‰ Score global : {quality['global_score']:.2f}\n")


    # RÃ©sumÃ©
    print("\nğŸ§¾ RÃ‰SUMÃ‰ FINAL -------------------")
    print(f"ğŸ™ï¸ Fichier : {file_path}")
    print(f"ğŸ§© {len(segments)} segment(s) analysÃ©(s)")
    print(f"ğŸ§± {len(user_stories)} User Stories gÃ©nÃ©rÃ©e(s)\n")
    for i, us in enumerate(user_stories, 1):
        print(f"{i}. ğŸ§± [{us['theme']}] {us['title']}")
        print(f"   ğŸ—£ï¸ IdÃ©e : {us['idea']}")
        print(f"   â­ PrioritÃ© : {us['priority']}\n")

    print("âœ… Pipeline audio multi-intervenants terminÃ©.")
    return user_stories
